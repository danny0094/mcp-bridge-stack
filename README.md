# ğŸ§© AnythingLLM Local MCP Framework
A modular, local plugin system that enables **AnythingLLM** to communicate with multiple **MCP servers** running in isolated Docker containers.  
It provides a single, secure HTTP access point while keeping every MCP fully sandboxed and independent.

---

Installation:

`git clone https://github.com/danny0094/mcp-bridge-stack
cd mcp-bridge-stack
docker compose up -d
docker ps
`


## ğŸš€ Overview

This framework extends AnythingLLM with a **Claude-like local MCP environment**, built entirely with Docker containers.

### âœ¨ Features
- **Safe & clean:** No host or Docker-in-Docker access.
- **Modular:** Each MCP runs in its own lightweight FastAPI container.
- **Bridge-controlled:** AnythingLLM communicates only with the `mini-bridge`, never directly with MCPs.
- **Dynamic registry:** MCP servers are auto-loaded from `mcp_registry.json`.
- **Auto reload:** Configuration changes are applied live â€“ no container restart required.
- **Example MCPs:** Includes `Dummy-MCP` (handshake test) and a working `MCP-Time` module returning system time.

---

## ğŸ§  Architecture Diagram

```text
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Docker Network: danny-ai-net
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚
â”œâ”€ anythingllm  (Port 3001)
â”‚     â†³ Main app (UI + chat logic)
â”‚     â†³ Sends all queries â†’ mini-bridge
â”‚
â”œâ”€ mini-bridge  (Port 4100)
â”‚     â†³ Central gateway between AnythingLLM and all MCPs
â”‚     â†³ Routes requests via Decision Model
â”‚     â†³ Auto-reloads mcp_registry.json dynamically
â”‚
â”‚     Workflow:
â”‚        User â†’ AnythingLLM â†’ mini-bridge
â”‚              â”œâ”€â†’ Decision-Agent (/route)
â”‚              â”‚      â†³ Decides which MCP to call
â”‚              â”‚      â†³ Returns JSON { "tool": "time" }
â”‚              â””â”€â†’ Forwards to chosen MCP via MCP-Hub
â”‚
â”œâ”€ decision-agent (prompt_injector)  (Port 4300)
â”‚     â†³ Lightweight model (Gemma 270M)
â”‚     â†³ Analyses prompt context
â”‚     â†³ Decides when to trigger MCP calls
â”‚     â†³ Responds with structured tool decision
â”‚
â”œâ”€ mcp-hub  (Port 4400)
â”‚     â†³ Central registry + router for all MCP modules
â”‚     â†³ Provides endpoints:
â”‚          â€¢ /list â€“ list all MCPs
â”‚          â€¢ /status/<id> â€“ health check
â”‚          â€¢ /route â€“ forward via hub
â”‚
â”œâ”€ dummy-mcp  (Port 4200)
â”‚     â†³ Test MCP â€“ returns handshake / status info
â”‚
â”œâ”€ mcp-time  (Port 4210)
â”‚     â†³ Returns current UTC or local time via JSON-RPC
â”‚
â””â”€ (future) mcp-docs / mcp-weather / ...
      â†³ Extend system modularly â€“ new MCPs auto-register in hub
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚
â””â”€ Dual Model Flow:
       Decision Model  ğŸ§­ (Gemma 270M) â†’ Tool choice
       Main Model      ğŸ§  (Gemma 12B / DeepSeek 8B) â†’ Reasoning + response
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

---

## âš™ï¸ File Overview

### `mini_bridge.py`
Core gateway component forwarding AnythingLLM requests to the appropriate MCP.

```python
REGISTRY_PATH = "/app/config/mcp_registry.json"
CHECK_INTERVAL = 3  # seconds for auto reload
```

This file dynamically loads all active MCP servers from `config/mcp_registry.json`.

**Decision forwarding:**
```python
decision = await client.post("http://decision-agent:4300/route", json=payload)
route = decision.json().get("tool", "dummy")  # fallback
```
This defines how the bridge communicates with the Decision-Agent (port 4300 as example).

---

### `mini_prompt_injector.py`
Acts as the **Decision Model Agent**. It intercepts messages and determines which MCP should be triggered.

**Environment variables:**
```python
OLLAMA_URL = os.getenv("OLLAMA_URL", "http://ollama:11434/api/chat")
MCP_HUB_URL = os.getenv("MCP_HUB_URL", "http://host.docker.internal:4000")
```

- `OLLAMA_URL` â†’ your Ollama server (must end with `/api/chat`)
- `MCP_HUB_URL` â†’ internal URL for MCP-Hub access (same network)

**Model configuration:**
```python
DECISION_MODEL = "gemma3:270m"
ANSWER_MODEL   = "gemma3:12b"
```
- **Decision Model** â†’ chooses the right MCP tool.
- **Answer Model** â†’ generates final reasoning and response.

**System prompt:**  
Customizable prompt automatically passed to the Answer Model.  
Use it to define behavioral rules, tone, or response formatting.

---

### `mcp_hub.py`
Central registry and router connecting all MCP servers.

When adding a new MCP container, simply extend the `TOOLS` dictionary:

```python
TOOLS = {
    "time": os.getenv("MCP_TIME_URL", "http://host.docker.internal:4210/"),
    "weather": os.getenv("MCP_WEATHER_URL", "http://host.docker.internal:4220/"),
    "docs": os.getenv("MCP_DOCS_URL", "http://host.docker.internal:4230/")
}
```

Each new entry adds a live MCP endpoint visible via `/list` and `/status`.

---

## ğŸ§© Dual-Model System

| Layer | Model | Role | Description |
|-------|--------|------|--------------|
| ğŸ§­ Decision Layer | Gemma 3 270M | Routing / Tool selection | Analyzes prompt context and decides which MCP to call |
| ğŸ§  Main Layer | Gemma 3 12B / DeepSeek R1 8B | Reasoning / Response | Generates the final user-facing answer |

---

## ğŸ’¡ Example Use Case

> â€œHow late is it?â€ â†’ AnythingLLM â†’ mini-bridge â†’ Decision-Agent â†’ MCP-Time â†’ Response returned via Main Model.

---

## ğŸ§± Future Modules

You can easily extend the system with more MCPs:
- `mcp-weather` â†’ fetch local weather data  
- `mcp-docs` â†’ handle document search  
- `mcp-audio` â†’ transcribe or process voice inputs  

Each additional MCP only needs:
1. Its own small FastAPI container.
2. Entry in `mcp_registry.json` or `mcp_hub.py`.
3. Network label in `docker-compose.yml`.

---

## ğŸ§° Credits & Notes

Built by **Danny** (2025) as a local AnythingLLM extension for modular AI experimentation.  
Inspired by Claudeâ€™s MCP design, but built entirely for local, offline use.

---

ğŸ“„ *For documentation or updates, visit the projectâ€™s GitHub page.*
